<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chenyi Zhuang</title>

    <meta name="author" content="Chenyi Zhuang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	  
    <link rel="icon" href="images/favicon/kitty.png" type="image/x-icon">
<!--     <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--     <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Oxanium:wght@200..800&display=swap" rel="stylesheet"> -->
    <style>
	@import url('https://fonts.googleapis.com/css2?family=Oxanium:wght@200..800&display=swap');
    </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chenyi Zhuang
                </p>
                <p> I'm currently a master's graduate student in Computer Technology at the Nanjing University of Aeronautics and Astronautics (2022-2025), supervised by Prof. Pan Gao at the Immersive and Interactive Multimedia Lab (<a href="https://i2-multimedia-lab.github.io">I2ML</a>).
<!-- 			I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>. -->
                </p>
                <p> I am applying for a PhD program in the fall of 2025. If you are interested in me, feel free to contact me by e-mail.
<!--                   At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:chenyi.zhuang@nuaa.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/Chenyi_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=To5gjDQAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Chenyi-Zhuang/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Chenyi.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Chenyi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, especially generative models, and explainable AI. My current research primarily focuses on image-related (multi-modal) tasks, but I am also interested in computer graphics and video understanding.
                </p>
                <p>
                  <strong>* indicate an equal contribution.</strong>
                </p>
              </td>
            </tr>
          </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
		  
	    <tr>
	      <td style="padding:20px;width:25%;vertical-align:middle">
	          <img src='images/diffuseST.jpg' width=100%>
	        </div>
	      </td>
	      <td style="padding:20px;width:75%;vertical-align:middle">
		<span class="papertitle">DiffuseST: Unleashing the Capability of the Diffusion Model for Style Transfer</span>
	        <br>
		<span style="font-family:Oxanium;">Ying Hu*</span>,
		<span style="font-family:Oxanium;">Chenyi Zhuang*</span>,
		<span style="font-family:Oxanium;">Pan Gao</span>.
	        <em>ACM MM Asia</em>, 2024
	        <br>
	        <a href="https://github.com/I2-Multimedia-Lab/DiffuseST">code</a>
	        /
	        <a href="https://arxiv.org/abs/2410.15007">arXiv</a>
	        <p></p>
	        <p>
		Leverage textual and spatial representations and the step-by-step denoising nature of the pre-trained diffusion model to achieve balanced style transfer results.
		</p>
	      </td>
	    </tr>
	
	    <tr>
	      <td style="padding:20px;width:25%;vertical-align:middle">
	          <img src='images/magnet.png' style="width:100%;text-align:center;">
	      </td>
	      <td style="padding:20px;width:75%;vertical-align:middle">
	        
		<span class="papertitle">Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function </span>
	        
	        <br>
		<span style="font-family:Oxanium;color:#eb507e">Chenyi Zhuang</span>,
		<span style="font-family:Oxanium;color:#666666">Ying Hu</span>,
		<span style="font-family:Oxanium;color:#666666">Pan Gao</span>.
	        <em>NeurIPS</em>, 2024
	        <br>
	        <a href="https://github.com/I2-Multimedia-Lab/Magnet">code</a>
	        /
	        <a href="https://arxiv.org/abs/2409.19967">arXiv</a>
	        <p></p>
	        <p>
		In-depth analysis of attribute understanding for CLIP text encoder and CLIP-based diffusion models, a novel training-free approach to tackle the attribute binding issue.
	        </p>
	      </td>
	    </tr>

		  
	    <tr>
	      <td style="padding:20px;width:25%;vertical-align:middle">
	          <img src='images/cdformer.png' width=100%>
	        </div>
	      </td>
	      <td style="padding:20px;width:75%;vertical-align:middle">
		<span class="papertitle">CDFormer: When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution</span>
	        <br>
		<span style="font-family:Oxanium;">Qingguo Liu</span>,
		<span style="font-family:Oxanium;">Chenyi Zhuang</span>,
		<span style="font-family:Oxanium;">Pan Gao</span>,
		<span style="font-family:Oxanium;">Jie Qin</span>.
	        <em>CVPR</em>, 2024
	        <br>
	        <a href="https://github.com/I2-Multimedia-Lab/CDFormer">code</a>
	        /
	        <a href="https://arxiv.org/abs/2405.07648">arXiv</a>
	        <p></p>
	        <p>
		Investigate the diffusion model as an estimator to predict Content Degradation Prior (CDP) with rich content detail for the super-resolution task.
		</p>
	      </td>
	    </tr>
	
	
	    <tr>
	      <td style="padding:20px;width:25%;vertical-align:middle">
	          <img src='images/styleprompter.jpg' style="width:100%;text-align:center;">
	      </td>
	      <td style="padding:20px;width:75%;vertical-align:middle">
		<span class="papertitle">StylePrompter: All Styles Need Is Attention</span>
	        <br>
		<span style="font-family:Oxanium;">Chenyi Zhuang</span>,
		<span style="font-family:Oxanium;">Pan Gao</span>,
		<span style="font-family:Oxanium;">Aljosa Smolic</span>.
		      
	        <em>ACM MM</em>, 2023
	        <br>
	        <a href="https://github.com/I2-Multimedia-Lab/StylePrompter">code</a>
	        /
	        <a href="https://arxiv.org/abs/2307.16151">arXiv</a>
	        <p></p>
	        <p>
		Propose a Transformer-based framework to predict W+ codes at the token level for StyleGAN, with a Style-driven Multi-scale Adaptive Refinement Transformer (SMART) block to refine features in F space.
	        </p>
	      </td>
	    </tr>

			  
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>What's More?</h2>
                <p>
                  My academic career is guided by two principles: <strong>"A believing heart is magic"</strong> which enables my commitment to any prospective work and project, and <strong>"simple but effective"</strong>  which reflects my high productivity and creativity. </p>
		</p>
              </td>
            </tr>
          </tbody></table>
		
            
          </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                01/11/2024 Updated
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      	</tr>
    </table>
  </body>
</html>
